{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**\n",
    "* **single Julia process → multiple Julia processes** that coordinate to perform certain computations\n",
    "\n",
    "**Why**\n",
    "* **Scaling things up**: run computations on multiple CPU cores, potentially even on different machines, e.g. nodes of a supercomputer or a local cluster of desktop machines.\n",
    "* Effectively increase your total memory, e.g. to process a large dataset that wouldn't fit into local memory.\n",
    "\n",
    "**Julia provides two fundamental implementations and paradigms**\n",
    "* Julia's built-in [Distributed.jl](https://docs.julialang.org/en/v1/stdlib/Distributed/) standard library\n",
    "* [Message Passing Interface (MPI)](https://www.mpi-forum.org/) through [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed.jl (standard library) vs MPI\n",
    "\n",
    "**Distributed.jl**\n",
    "* convenient for **\"ad-hoc\" distributed computing** (e.g. data processing)\n",
    "* intuitive **master-worker model** (often naturally aligns with the structure of scientific computations)\n",
    "* \"one-sided\" communication\n",
    "* **interactivity**, e.g. in a REPL / in Jupyter\n",
    "* built-in, no external setup necessary\n",
    "* higher overhead than MPI and doesn't scale as well (by default doesn't utilizie Infiniband/OmniPath)\n",
    "\n",
    "**MPI**\n",
    "* **de-facto industry standard** for massively parallel computing, e.g. large scale distributed computing\n",
    "* **known to scale well** up to thousands of compute nodes\n",
    "* Single Program Multiple Data (SPMD) programming model (can be more challenging at first)\n",
    "* **No (or very poor) interactivity** (see [MPIClusterManagers.jl](https://github.com/JuliaParallel/MPIClusterManagers.jl) or [tmpi](https://github.com/Azrael3000/tmpi))\n",
    "\n",
    "\n",
    "The focus of this notebook is on **Distributed.jl** (MPI → later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed.jl programming model\n",
    "\n",
    "<img src=\"./imgs/distributedjl_model.svg\" width=850>\n",
    "\n",
    "**Master-worker paradigm**:\n",
    "* One master process coordinates a set of worker processes (that eventually perform computations).\n",
    "* Programmer only controls the master directly. The workers are \"instructed\" (**one-sided** communication).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and removing workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers() # the master is considered a worker as long as there are no other workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the number of workers, i.e. Julia processes, we can call **`addprocs`**.\n",
    "\n",
    "(Alternatively, one can set the number of worker processes when starting julia with the `-p` option. E.g. `julia -p 4` gives 5 processes, 1 master and 4 workers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every process has a Julia internal `pid` (process id). **The master always has pid 1.** You can get the worker pids from `workers()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove workers, we can pass an array of pids to `rmprocs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmprocs(workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nworkers() # only the master is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One master to rule them all - `@spawn`, `@spawnat`, `@fetch`, `@fetchfrom`, `@everywhere`..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit computations to any worker we can use the following macro\n",
    "\n",
    "* `@spawn`: run a command or a code block on any worker and return a `Future` (a wrapped `Task`) to it's result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the conceptual similarity between `Threads.@spawn` (task → thread) and `Distributed.@spawn` (task → process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@spawn 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = @spawn 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@fetch` does the same, just in one command. It has **blocking** semantics and only returns once the result has arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch 3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch rand(3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which worker did the work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch begin\n",
    "    println(myid());\n",
    "    3+3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we understood all that, let's delegate a *complicated* calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function complicated_calculation()\n",
    "    sleep(1) # so complex that it takes a long time :)\n",
    "    randexp(5)\n",
    "end\n",
    "\n",
    "@fetch complicated_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Every worker is a separate Julia process.** (Think of having multiple Julia REPLs open at once.)\n",
    "\n",
    "We only defined `complicated_calculation()` on the master process. The function doesn't exist on any of the workers.\n",
    "\n",
    "The macro `@everywhere` allows us to perform steps on all processes (master and worker). This is particularly useful for loading packages and defining functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin # execute this block on all workers\n",
    "    using Random\n",
    "    \n",
    "    function complicated_calculation()\n",
    "        sleep(1)\n",
    "        randexp(5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fetch complicated_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending messages and moving data typically constitute most of the overhead in a distributed program. **Reducing the number of messages and the amount of data sent is critical to achieving performance and scalability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is **implicitly** transferred to a worker as part of a task that needs it. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function method1()\n",
    "    A = rand(100,100)\n",
    "    B = rand(100,100)\n",
    "    C = @fetch A^2 * B^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` and `B` are created on the master process, transferred to a worker, squared and multiplied by the worker before the result is finally transferred back to the master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to this similar implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function method2()\n",
    "    C = @fetch rand(100,100)^2 * rand(100,100)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the entire computation happens on a worker process. Only the result is transferred to the master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "@btime method1();\n",
    "@btime method2();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more tools for explicit data movement like `Channels`/`RemoteChannels` and [ParallelDataTransfer.jl](https://github.com/ChrisRackauckas/ParallelDataTransfer.jl/). For the sake of brevity, we will not cover them here. (See `notebooks/backup` for basic peer-to-peer communication via `RemoteChannels`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Fortunately, many useful parallel computations do not require (much) data movement at all. A common example is a direct Monte Carlo simulation, where multiple processes can handle independent simulation trials simultaneously. → **montecarlo_pi** exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level tools: `@distributed` and `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen some of the fundamental building blocks for distributed computing with Distributed.jl. However, in practice, one wants to think as little as possible about how to distribute the work and explicitly spawn tasks in parallel programs.\n",
    "\n",
    "Julia provides **high-level convenience** tools to\n",
    " * distribute `map` $\\quad$ → $\\quad$ [`pmap`](https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.pmap)\n",
    " * distribute loops $\\quad$ → $\\quad$ [`@distributed`](https://docs.julialang.org/en/v1/stdlib/Distributed/#Distributed.@distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel map: `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often, one simply wants to parallize a `map` operation where a function `f` is applied to all elements of a collection. This is a typical instance of **data parallelism**, which covers a vast class of compute-intensive programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(x->x^2, 1:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a pattern can be parallelized in Julia via the high-level function `pmap` (\"parallel map\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Singular values of multiple matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using LinearAlgebra\n",
    "\n",
    "M = Matrix{Float64}[rand(200,200) for i = 1:10]; # array holding 10 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this indeed utilized multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap(M) do m\n",
    "    println(myid())\n",
    "    svdvals(m)\n",
    "end\n",
    "\n",
    "# do synax:\n",
    "#\n",
    "# pmap(M) do m\n",
    "#     ...\n",
    "# end\n",
    "#\n",
    "# is the same as\n",
    "#\n",
    "# pmap(m -> ..., M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime map($svdvals, $M);\n",
    "@btime pmap($svdvals, $M);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed loops (`@distributed`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Reduction\n",
    "\n",
    "Reductions (e.g. `sum`) can't be expressed with `pmap` (e.g. $\\mathbb{R}^n \\rightarrow \\mathbb{R}$ vs $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$).\n",
    "\n",
    "Task: Counting heads in a series of coin tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_loop(n)\n",
    "    c = 0\n",
    "    for i = 1:n\n",
    "        c += rand((0,1))\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "N = 200_000_000\n",
    "@btime count_heads_loop($N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here, `+` is the **reducer function**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize this reduction using\n",
    "* `@distributed (reducer function) for ...`.\n",
    "\n",
    "Note that this has **blocking** character and returns the result once it has arrived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_distributed_loop(n)\n",
    "    c = @distributed (+) for i in 1:n\n",
    "        rand((0,1))\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime count_heads_distributed_loop($N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed version is about **4x faster**, which is all we could hope for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `@threads :static`, `@distributed` distributes the work **evenly** among the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function count_heads_distributed_verbose(n)\n",
    "    c = @distributed (+) for i in 1:n\n",
    "        x = rand((0,1))\n",
    "        println(x)\n",
    "        x\n",
    "    end\n",
    "    c\n",
    "end\n",
    "\n",
    "count_heads_distributed_verbose(8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Example: Array mutation (if time permits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from `@distributed (reducer) ...` there also is a `@distributed for ...` form. The latter is **non-blocking** and returns a `Task`. (You can think of it as a distributed version of `@spawn` for all the iterations.)\n",
    "\n",
    "However, since the loop body will be executed on different processes, one must be careful to operate on **data structures that are available on all processes** (similar to the mistake highlighted above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function square_broken()\n",
    "    A = collect(1:10)\n",
    "    @sync @distributed for i in eachindex(A)\n",
    "        A[i] = A[i]^2\n",
    "    end\n",
    "    return A\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_broken()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually make all processes operate on the same array, one can use a `SharedArray`. For this to work, the **processes need to live on the same machine**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using SharedArrays # must be loaded everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = SharedArray(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function square!(X)\n",
    "    for i in eachindex(X)\n",
    "        sleep(0.001) # mimic some computational cost\n",
    "        X[i] = X[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "function square_distributed!(X)\n",
    "    @sync @distributed for i in eachindex(X)\n",
    "        sleep(0.001) # mimic some computational cost\n",
    "        X[i] = X[i]^2\n",
    "    end\n",
    "end\n",
    "\n",
    "A = rand(10,10)\n",
    "S = SharedArray(A)\n",
    "\n",
    "@btime square!($A);\n",
    "@btime square_distributed!($S);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the functions `square!` and `square_distributed!` could have also been written with `map` and `pmap`, respectively. (data parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `@distributed` vs `pmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's `pmap` is designed for the case where\n",
    "\n",
    "* one wants to apply **a function to a collection**,\n",
    "* one needs **load-balancing** (e.g. workload is non-uniform), and/or\n",
    "* each function call does enough work to amortize the overhead. \n",
    "\n",
    "On the other hand, `@distributed` is good for\n",
    "\n",
    "* **reductions** (can't be written as `map`/`pmap`),\n",
    "* loops where each iteration **takes about the same time** (uniform workload), and/or\n",
    "* loops where the workload of each iteration is (very) small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## High-level array abstractions: [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl)\n",
    "\n",
    "`DArray`: each process has local access to just a chunk of the data, and no two processes share the same chunk.\n",
    "\n",
    "* `pmap(x::Array)`: parallel function on regular data structure\n",
    "* `map(x::DArray)`: regular function on parallel data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed, BenchmarkTools; rmprocs(workers()); addprocs(4); # clean reset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using DistributedArrays, LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Matrix{Float64}[rand(200,200) for i = 1:10];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = distribute(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which part does each worker hold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in workers()\n",
    "    println(@fetchfrom p localindices(D))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime map($svdvals, $M) samples=5 evals=3;\n",
    "@btime map($svdvals, $D) samples=5 evals=3;\n",
    "@btime pmap($svdvals, $M) samples=5 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating workers on other machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting worker processes (via `addprocs`) is handled by [ClusterManagers](https://docs.julialang.org/en/v1/manual/distributed-computing/#ClusterManagers).\n",
    "\n",
    "* The default is `LocalManager`. It is automatically used when running `addprocs(i::Integer)`.\n",
    "* Another manager is `SSHManager`. It is automatically used when running `addprocs(hostnames::Array)`, e.g. `addprocs([\"node123\", \"node456\"])`. The only requirement is a **passwordless ssh access** to all specified hosts.\n",
    "* Cluster managers for SLURM, PBS, and others are provided in [ClusterManagers.jl](https://github.com/JuliaParallel/ClusterManagers.jl). For SLURM, this will make `addprocs` use `srun` under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example*\n",
    "\n",
    "```julia\n",
    "using Distributed\n",
    "\n",
    "addprocs([\"node123\", \"node123\"])\n",
    "\n",
    "@everywhere println(gethostname())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also start multiple processes on different machines:\n",
    "```julia\n",
    "addprocs([(\"node123\", 2), (\"node456\", 3)]) # starts 2 workers on node123 and 3 workers on node456\n",
    "\n",
    "# Use :auto to start as many processes as CPU threads are available\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Be aware of different paths:**\n",
    "* By default, `addprocs` expects to find the julia executable on the remote machines under the same path as on the host (master).\n",
    "* It will also try to `cd` to the same folder (set the working directory).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from `?addprocs`, `addprocs` takes a bunch of keyword arguments, two of which are of particular importance in this regard:\n",
    "\n",
    "* `dir`: working directory for the worker processes\n",
    "* `exename`: path to julia executable (potentially augmented with pre-commands) for the worker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "rmprocs(workers())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia (1 thread) 1.10.0",
   "language": "julia",
   "name": "julia-_1-thread_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
