{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: SAXPY on NVIDIA A100 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement two GPU-variants of the **SAXPY** kernel (`y[i] = a * x[i] + y[i]`):\n",
    "\n",
    "1) A version using array abstractions, i.e. `CuArrays` and simple broadcasting.\n",
    "2) A hand-written SAXPY CUDA kernel.\n",
    "\n",
    "Afterwards, you'll benchmark the performance of the variants and compare it to the CUBLAS implementation by NVIDIA (that ships with CUDA). Since SAXPY is memory bound, we'll consider the achieved memory bandwidth (GB/s) as the performance metric.\n",
    "\n",
    "The exercise tasks are marked in the code cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using BenchmarkTools\n",
    "using PrettyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_broadcast_gpu!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Computes the SAXPY via broadcasting\"\n",
    "function saxpy_broadcast_gpu!(a, x, y)\n",
    "    # --------\n",
    "    #\n",
    "    # Task 1: Use broadcasting to implement a SAXPY kernel. Since we will\n",
    "    #         run the kernel on the GPU, don't forget to synchronize!\n",
    "    #\n",
    "    # --------\n",
    "    CUDA.@sync y .= a .* x .+ y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_saxpy_kernel!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"CUDA kernel for computing SAXPY on the GPU\"\n",
    "function _saxpy_kernel!(a, x, y)\n",
    "    # --------\n",
    "    #\n",
    "    # Task 2: Define the \"scalar\" SAXPY kernel here. Make sure to check that\n",
    "    #         the global index `i` is within the bounds of `y` (and `x`).\n",
    "    #\n",
    "    # --------\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(y)\n",
    "        @inbounds y[i] = a * x[i] + y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cuda_kernel!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Computes SAXPY on the GPU using the custom CUDA kernel `_saxpy_kernel!`\"\n",
    "function saxpy_cuda_kernel!(a, x, y; nthreads, nblocks)\n",
    "    # --------\n",
    "    #\n",
    "    # Task 3: Use the `@cuda` macro to run the kernel defined above (`_saxpy_kernel!`).\n",
    "    #         Spawn the kernel with `nthreads` many threads and `nblocks` many blocks.\n",
    "    #         Don't forget to synchronize :)\n",
    "    #\n",
    "    # --------\n",
    "    CUDA.@sync @cuda(threads=nthreads,\n",
    "                     blocks=nblocks,\n",
    "                     _saxpy_kernel!(a, x, y))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_cublas!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Computes SAXPY using the CUBLAS function `CUBLAS.axpy!` provided by NVIDIA\"\n",
    "function saxpy_cublas!(a, x, y)\n",
    "    CUDA.@sync CUBLAS.axpy!(length(x), a, x, y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy_gpu_bench (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Computes the GFLOP/s from the vector length `len` and the measured runtime `t`.\"\n",
    "saxpy_flops(t; len) = 2.0 * len * 1e-9 / t # GFLOP/s\n",
    "\n",
    "\"Computes the GB/s from the vector length `len`, the vector element type `dtype`, and the measured runtime `t`.\"\n",
    "saxpy_bandwidth(t; dtype, len) = 3.0 * sizeof(dtype) * len * 1e-9 / t # GB/s\n",
    "\n",
    "function saxpy_gpu_bench()\n",
    "    if !contains(lowercase(name(device())), \"a100\")\n",
    "        @warn(\"This script was tuned for a NVIDIA A100 GPU. Your GPU: $(name(device())).\")\n",
    "    end\n",
    "    dtype = Float32\n",
    "    nthreads = 1024\n",
    "    nblocks = 500_000\n",
    "    len = nthreads * nblocks # vector length\n",
    "    a = convert(dtype, 3.1415)\n",
    "    xgpu = CUDA.ones(dtype, len)\n",
    "    ygpu = CUDA.ones(dtype, len)\n",
    "\n",
    "    t_broadcast_gpu = @belapsed saxpy_broadcast_gpu!($a, $xgpu, $ygpu) samples=10 evals=2\n",
    "    t_cuda_kernel = @belapsed saxpy_cuda_kernel!($a, $xgpu, $ygpu; nthreads = $nthreads,\n",
    "                                                 nblocks = $nblocks) samples=10 evals=2\n",
    "    t_cublas = @belapsed saxpy_cublas!($a, $xgpu, $ygpu) samples=10 evals=2\n",
    "    times = [t_broadcast_gpu, t_cuda_kernel, t_cublas]\n",
    "\n",
    "    flops = saxpy_flops.(times; len)\n",
    "    bandwidths = saxpy_bandwidth.(times; dtype, len)\n",
    "\n",
    "    labels = [\"Broadcast\", \"CUDA kernel\", \"CUBLAS\"]\n",
    "    data = hcat(labels, 1e3 .* times, flops, bandwidths)\n",
    "    pretty_table(data;\n",
    "                 header = ([\"Variant\", \"Runtime\", \"FLOPS\", \"Bandwidth\"],\n",
    "                           [\"\", \"ms\", \"GFLOP/s\", \"GB/s\"]))\n",
    "    println(\"Theoretical Memory Bandwidth of NVIDIA A100: 1555 GB/s\")\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬─────────┬─────────┬───────────┐\n",
      "│\u001b[1m     Variant \u001b[0m│\u001b[1m Runtime \u001b[0m│\u001b[1m   FLOPS \u001b[0m│\u001b[1m Bandwidth \u001b[0m│\n",
      "│\u001b[90m             \u001b[0m│\u001b[90m      ms \u001b[0m│\u001b[90m GFLOP/s \u001b[0m│\u001b[90m      GB/s \u001b[0m│\n",
      "├─────────────┼─────────┼─────────┼───────────┤\n",
      "│   Broadcast │ 4.92587 │ 207.882 │   1247.29 │\n",
      "│ CUDA kernel │ 4.53087 │ 226.005 │   1356.03 │\n",
      "│      CUBLAS │  4.5067 │ 227.217 │    1363.3 │\n",
      "└─────────────┴─────────┴─────────┴───────────┘\n",
      "Theoretical Memory Bandwidth of NVIDIA A100: 1555 GB/s\n"
     ]
    }
   ],
   "source": [
    "# --------\n",
    "#\n",
    "# Task 4: Run the benchmark and interpret the results.\n",
    "#         How does the performance of the different variants compare?\n",
    "#\n",
    "# --------\n",
    "saxpy_gpu_bench()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (1 thread) 1.10.0",
   "language": "julia",
   "name": "julia-_1-thread_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
