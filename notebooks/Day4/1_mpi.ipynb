{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What**\n",
    "* **single Julia process → multiple Julia processes** that coordinate to perform certain computations\n",
    "\n",
    "**Why**\n",
    "* **Scaling things up**: run computations on multiple CPU cores, potentially even on different machines, e.g. nodes of a supercomputer or a local cluster of desktop machines.\n",
    "* Effectively increase your total memory, e.g. to process a large dataset that wouldn't fit into local memory.\n",
    "\n",
    "**Julia provides two fundamental implementations and paradigms**\n",
    "* Julia's built-in [Distributed.jl](https://docs.julialang.org/en/v1/stdlib/Distributed/) standard library\n",
    "* [Message Passing Interface (MPI)](https://www.mpi-forum.org/) through [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed.jl (standard library) vs MPI\n",
    "\n",
    "**Distributed.jl**\n",
    "* convenient for **\"ad-hoc\" distributed computing** (e.g. data processing)\n",
    "* intuitive **master-worker model** (often naturally aligns with the structure of scientific computations)\n",
    "* \"one-sided\" communication\n",
    "* **interactivity**, e.g. in a REPL / in Jupyter\n",
    "* built-in, no external setup necessary\n",
    "* higher overhead than MPI and doesn't scale as well (by default doesn't utilizie Infiniband/OmniPath)\n",
    "\n",
    "**MPI**\n",
    "* **de-facto industry standard** for massively parallel computing, e.g. large scale distributed computing\n",
    "* **known to scale well** up to thousands of compute nodes\n",
    "* Single Program Multiple Data (SPMD) programming model (can be more challenging at first)\n",
    "* **No (or very poor) interactivity** (see [MPIClusterManagers.jl](https://github.com/JuliaParallel/MPIClusterManagers.jl) or [tmpi](https://github.com/Azrael3000/tmpi))\n",
    "\n",
    "\n",
    "The focus of this notebook is on **MPI** (Distributed.jl → later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Computing: Message Passing Interface (MPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing messages (i.e. moving data between processes) can be very costly in a distributed program. Reducing the number of messages and the amount of data sent is critical to achieving performance and scalability. For these reasons, MPI is centered around message passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI and MPI.jl\n",
    "\n",
    "* **[MPI](https://www.mpi-forum.org/)**: A [standard](https://www.mpi-forum.org/docs/) with several specific implementations (e.g. [OpenMPI](https://www.open-mpi.org/), [IntelMPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html#gs.73krlr), [MPICH](https://www.mpich.org/))\n",
    "* **[MPI.jl](https://github.com/JuliaParallel/MPI.jl)**: Julia package and interface to (most) MPI implementations ([paper](https://proceedings.juliacon.org/papers/10.21105/jcon.00068))\n",
    "\n",
    "\n",
    "\n",
    "### MPI programming model\n",
    "\n",
    "<img src=\"./imgs/mpi_model.svg\" width=850>\n",
    "\n",
    "**Sinlge Program Multiple Data (SPMD):**\n",
    "* **all processes execute the same code** but have different IDs (rank).\n",
    "* **conditionals can be used to simulate different behavior (MPMD)**\n",
    "* individual processes flow at there own pace, **they can (and will) get out of sync**\n",
    "* selecting the concrete number of processes is deferred to \"runtime\"\n",
    "\n",
    "**Message passing:**\n",
    "* **Two-sided communication:** explicit `Send` and explicit `Recv`\n",
    "* Conceptually, a message is a tuple (memory address, size, datatype) (e.g. a `Vector{Float64}`)\n",
    "\n",
    "### Basic example: Hello World\n",
    "\n",
    "```julia\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "comm   = MPI.COMM_WORLD\n",
    "rank   = MPI.Comm_rank(comm)\n",
    "nranks = MPI.Comm_size(comm)\n",
    "\n",
    "println(\"Hello world, I am rank $rank of $nranks\")\n",
    "\n",
    "MPI.Finalize()\n",
    "\n",
    "# see mpi_examples/1_mpi_hello.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental MPI functions\n",
    "\n",
    "* `MPI.Init()` and `MPI.Finalize()` (the latter isn't necessary in Julia)\n",
    "\n",
    "* `MPI.COMM_WORLD`: default *communicator*, includes all MPI ranks\n",
    "\n",
    "* `MPI.Comm_rank(comm)`: unique rank of the process calling this function (**MPI rank ids start at 0!**)\n",
    "\n",
    "* `MPI.Comm_size(comm)`: total number of ranks in the given communicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an MPI program\n",
    "\n",
    "#### Getting MPI\n",
    "\n",
    "By default, an appropriate MPI will be **automatically** downloaded when adding MPI.jl to a Julia environment (see e.g. MPICH_jll.jl)). Works out of the box more often than not.\n",
    "\n",
    "However, in particular on larger HPC clusters, one sometimes wants/needs to use a **system-wide MPI** installation. Potential reasons include:\n",
    "\n",
    "* Vendor-specific MPI required for MPI to work at all.\n",
    "* Fine-tuned MPI configuration necessary for best performance.\n",
    "* CUDA-aware or ROCm-aware MPI\n",
    "\n",
    "##### How to use a system MPI?\n",
    "\n",
    "```julia\n",
    "using MPIPreferences\n",
    "MPIPreferences.use_system_binary()\n",
    "```\n",
    "If you do this before adding MPI.jl, no MPI will be downloaded.\n",
    "\n",
    "For more, check out the [MPI.jl documentation](https://juliaparallel.org/MPI.jl/stable/configuration/#configure_system_binary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel startup: `mpiexecjl` driver\n",
    "\n",
    "MPI.jl provides [**`mpiexecjl`**](https://juliaparallel.org/MPI.jl/stable/configuration/#Julia-wrapper-for-mpiexec) that wraps, e.g., `mpirun`, `mpiexec`, or `srun` (configurable) to run you Julia MPI program. This enables\n",
    "* the use of different MPI implementations for different Julia environments/projects\n",
    "* the use of the default MPI that was installed by MPI.jl automatically\n",
    "\n",
    "**You should use the following command to run your MPI application:**\n",
    "\n",
    "```\n",
    "mpiexecjl --project -n N julia mycode.jl\n",
    "```\n",
    "\n",
    "Here, `N` is the desired number of MPI ranks.\n",
    "\n",
    "<img src=\"./imgs/julia_mpi_example.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment: MPI.jl vs MPI in C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI.jl is very similar to [mpi4py](https://mpi4py.readthedocs.io/en/stable/) for Python (see [this paper](https://proceedings.juliacon.org/papers/10.21105/jcon.00068)).\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Julia MPI functions typically have **less function arguments** than C counterparts (see below).\n",
    "* MPI.jl functions can often automatically register and handle **custom Julia types and functions** (see e.g. `mpi_examples/6_mpi_custom_reduction.jl`)\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "* Not every (exotic) function of the MPI API is wrapped yet.\n",
    "* Minor translation necessary when porting C/Fortran code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you really need to, you can use the \"low-level\" C-API via `MPI.API.*`, which is identical to what you might know from C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-to-point communication\n",
    "\n",
    "* **Sending:**\n",
    "    ```julia\n",
    "              MPI.Send(buf, comm; dest)\n",
    "              \n",
    "    ```\n",
    "  * `buf`: data buffer, typically an array\n",
    "  * `comm`: communicator\n",
    "  * `destination`: target rank (to receive the data)\n",
    "* **Receiving:**\n",
    "    ```julia\n",
    "              MPI.Recv!(recvbuf, comm; source=MPI.ANY_SOURCE)\n",
    "              \n",
    "    ```\n",
    "  * `recvbuf`: buffer to store the received data in, typically an array\n",
    "  * `comm`: communicator\n",
    "  * `source`: source rank (whos sending the data)\n",
    "\n",
    "These functions have **blocking semantics**, i.e. the calling process waits for the operation to \"complete\"!\n",
    "\n",
    "(Note that for sending, \"complete\" only means that the buffer can be reused but not necessarily that the full communication has happened (e.g. the message sending might have completed but the receiving hasn't started yet). A good explanation of the different MPI sending modes is available in [this stackoverflow post](https://stackoverflow.com/a/47041382/2365675).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "<img src=\"./imgs/mpi_sendrecv.svg\" width=300px>\n",
    "\n",
    "```julia\n",
    "msg = fill(rank, 10) # Vector{Int64} of length 10 filled with rank id\n",
    "\n",
    "if rank != 0\n",
    "    MPI.Send(msg, comm; dest=0) # blocking\n",
    "else\n",
    "    println(msg)\n",
    "    for r in 1:world_size-1\n",
    "        MPI.Recv!(msg, comm; source=r) # blocking\n",
    "        println(msg)\n",
    "    end\n",
    "end\n",
    "\n",
    "# full example, see mpi_examples/2_mpi_blocking_communication.jl\n",
    "```\n",
    "\n",
    "### Deadlocks\n",
    "Note that blocking point-to-point communcation **can lead to deadlocks!**\n",
    "\n",
    "<img src=\"./imgs/mpi_deadlock.svg\" width=400px>\n",
    "\n",
    "```julia\n",
    "# ring topology, i.e. periodic boundary conditions\n",
    "left  = mod(rank - 1, nranks)\n",
    "right = mod(rank + 1, nranks)\n",
    "\n",
    "# Warning: deadlock!\n",
    "MPI.Recv!(msg, comm; source=left)\n",
    "MPI.Send(msg, comm; dest=right)\n",
    "\n",
    "# the following will never be reached\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "* Alternate order of send and receive on neighboring ranks.\n",
    "* `MPI.Sendrecv!`\n",
    "* Non-blocking communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-blocking communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "* Avoid deadlocks\n",
    "* Avoid serialization/sequentialization\n",
    "* Overlaping of communication with computations and/or other communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially the same function signatures as above, but different function names and different behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Sending:**\n",
    "    ```julia\n",
    "              req = MPI.Isend(buf, comm[, req]; dest)\n",
    "    ```\n",
    "* **Receiving:**\n",
    "    ```julia\n",
    "              req = MPI.Irecv!(recvbuf, comm[, req]; source)\n",
    "    ```\n",
    "    \n",
    "Each function returns a `MPI.Request` (`req`), which may be used for (blocking) waiting/testing operations (`MPI.Wait(req)`). Optionally, the request object may be preallocated and passed as a third argument.\n",
    "\n",
    "```julia\n",
    "# ring topology, i.e. periodic boundary conditions\n",
    "left  = mod(rank - 1, nranks)\n",
    "right = mod(rank + 1, nranks)\n",
    "\n",
    "# non-blocking communication → no deadlock\n",
    "req1 = MPI.Irecv!(msg, comm; source=left)\n",
    "req2 = MPI.Isend(msg, comm; dest=right)\n",
    "\n",
    "# blocking wait\n",
    "MPI.Waitall([req1, req2])\n",
    "\n",
    "# the following will be reached\n",
    "\n",
    "# full example, see mpi_examples/3_mpi_ring_communication_nonblocking.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collective communication\n",
    "\n",
    "Perform operations involving **all processes/ranks** within a communicator.\n",
    "\n",
    "* Types of collective operations\n",
    "  * **synchronization**: let all ranks wait until all have reached a synchronization point (*barrier*)\n",
    "  * **data movement**: one-to-many and many-to-many communications (broadcast, scatter, gather, all to all)\n",
    "  * **collective computation**: parallel reduction (e.g. summation) of rank-local information → result on the \"master\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronization\n",
    "\n",
    "* Example: **Barrier**\n",
    "    ```julia\n",
    "              MPI.Barrier(comm)\n",
    "    ```\n",
    "\n",
    "Rank stops execution until **all ranks have reached the barrier**.\n",
    "\n",
    "Exemplary use-case: time measurement.\n",
    "\n",
    "```julia\n",
    "MPI.Barrier(comm)                         # synchronize all ranks\n",
    "time_start = MPI.Wtime()                  # start the clock\n",
    "\n",
    "@time sleep(rank)                         # perform computations that take different amounts of time\n",
    "\n",
    "MPI.Barrier(comm)                         # synchronize all ranks\n",
    "elapsed_time = MPI.Wtime() - time_start   # stop the clock\n",
    "\n",
    "# full example: mpi_examples/4_mpi_wtime.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data movement\n",
    "\n",
    "* Example: **Broadcasting**\n",
    "    ```julia\n",
    "              MPI.Bcast!(buffer, comm; root=0)\n",
    "              \n",
    "    ```\n",
    "  * `buffer`: source buffer for the `root` rank, destination buffer for all other ranks\n",
    "  * `comm`: communicator\n",
    "  * `root`: source rank (holds the data to be broadcasted)\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/mpi_bcast.svg\" width=700>\n",
    "<br>\n",
    "\n",
    "Broadcasting can be implemented by means of the point-to-point communications → **exercise!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collective computation\n",
    "\n",
    "* Example: **Reduction**\n",
    "    ```julia\n",
    "              recvbuf = MPI.Reduce(sendbuf, op, comm; root=0)\n",
    "              \n",
    "    ```\n",
    "  * `sendbuf`: rank-local data buffer to be sent/reduced\n",
    "  * `op`: reduction operation (e.g. `+` or `min`)\n",
    "  * `comm`: communicator\n",
    "  * `root`: target rank (to receive the result)\n",
    "  * `recvbuf`: result of parallel reduction on `root` rank, `nothing` on all other ranks\n",
    "  \n",
    "<br>\n",
    "<img src=\"imgs/mpi_reduce.svg\" width=785>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplary use-case: Parallel **trapezoidal integration**.\n",
    "\n",
    "<img src=\"imgs/trapezoids.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "# full example: mpi_examples/5_mpi_trapezoidal_integration.jl\n",
    "\n",
    "\"Function to be integrated (from 0 to 1). The analytic result is π.\"\n",
    "f(x) = 4 * √(1 - x^2)\n",
    "\n",
    "\"Evaluate definite integral (from `a` to `b`) by using the trapezoidal rule.\"\n",
    "function integrate_trapezoidal(a, b, n, h)\n",
    "    y = (f(a) + f(b)) / 2.0\n",
    "    for i in 1:n-1\n",
    "        x = a + i * h\n",
    "        y = y + f(x)\n",
    "    end\n",
    "    return y * h\n",
    "end\n",
    "\n",
    "# compute local integration interval etc....\n",
    "\n",
    "# perform local integration\n",
    "res_loc = integrate_trapezoidal(a_loc, b_loc, n_loc, h)\n",
    "\n",
    "# parallel reduction\n",
    "res = MPI.Reduce(res_loc, +, comm)\n",
    "\n",
    "# print result\n",
    "if 0 == rank\n",
    "    @printf(\"π (numerical integration) ≈ %20.16f\\n\", res)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI + CUDA\n",
    "\n",
    "The auto-shipped Julia artifacts for MPI are not CUDA-aware → use system libraries.\n",
    "\n",
    "Experience:\n",
    "\n",
    "* the system setup isn't always trivial/stable\n",
    "* but using MPI + CUDA is easy\n",
    "\n",
    "```julia\n",
    "# Example: sending and receiving CuArrays\n",
    "send_mesg = CuArray{Float64}(undef, N)\n",
    "recv_mesg = CuArray{Float64}(undef, N)\n",
    "fill!(send_mesg, Float64(rank))\n",
    "\n",
    "# pass GPU buffers (CuArrays) into MPI functions\n",
    "MPI.Sendrecv!(send_mesg, dst, 0, recv_mesg, src, 0, comm)\n",
    "\n",
    "# Full examples, see mpi_examples/mpi_cuda_aware/*.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation:\n",
    "* [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) + [NVTX.jl](https://github.com/JuliaGPU/NVTX.jl) (for instrumentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/report1.png\" width=800px>\n",
    "\n",
    "(source: `notebooks/backup/mpi_profiling_nsys`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options:\n",
    "* [Extrae.jl](https://github.com/bsc-quantic/Extrae.jl) (beta)\n",
    "* [ScoreP.jl](https://github.com/JuliaPerf/ScoreP.jl) (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## High-level tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* [PartitionedArrays.jl](https://github.com/fverdugo/PartitionedArrays.jl): Data-oriented parallel implementation of partitioned vectors and sparse matrices needed in FD, FV, and FE simulations.\n",
    "* [Elemental.jl](https://github.com/JuliaParallel/Elemental.jl): A package for dense and sparse distributed linear algebra and optimization.\n",
    "* [PETSc.jl](): Suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. ([original website](https://petsc.org/release/))\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More? Good MPI resources\n",
    "\n",
    "* Great [MPI self-study materials](https://www.hlrs.de/training/self-study-materials/mpi-course-material) provided by HLRS (slides + exercises).\n",
    "* [Using MPI: Portable Parallel Programming with the Message-Passing Interface](https://mitpress.mit.edu/9780262527392/using-mpi/), by William Gropp, Ewing Lusk and Anthony Skjellum (book)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
