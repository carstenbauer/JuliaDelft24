{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the [top500](https://top500.org/lists/top500/2023/11/) systems have GPUs as accelerators and they are dominating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./imgs/cpu_gpu_fraction.svg\" width=768>\n",
    "\n",
    "**Source:** [J. Apostolakis et al., *Detector simulation challenges for future accelerator experiments.*, Frontiers in Physics 10 (2022)](https://doi.org/10.3389/fphy.2022.913510)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"./imgs/Julia-code-cpu-gpu.png\" width=768>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/gpu_topology.svg\" width=500px>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory)\n",
    "* **SM**: Streaming Multiprocessor\n",
    "\n",
    "Communication bottleneck:\n",
    "* Host-device bandwidth (PCIe): **31.5 GB/s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NVIDIA A100\n",
    "\n",
    "##### Streaming Multiprocessor\n",
    "\n",
    "Source: [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "<img src=\"./imgs/a100_SM.png\" width=512px>\n",
    "\n",
    "**An entire NVIDIA A100 has 108 of these SMs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU vs GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                   | CPU                               | GPU                                 |\n",
    "|:-----------------:|:---------------------------------:|:-----------------------------------:|\n",
    "| optimized for     | latency and per-core performance  | computational throughput            |\n",
    "| cores             | complex                           | rather simple                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak arithmetic performance\n",
    "\n",
    "|               | compute units   | maximum clock frequency [GHz] | FP32 peak performance [TFLOPS] |\n",
    "|:-------------:|:---------------:|:-----------------------------:|:------------------------------:|\n",
    "| AMD EPYC 7763 |  64 x86 cores   |  3.50                         |  5.0                           |\n",
    "| NVIDIA A100   | 6912 CUDA cores |  1.41                         | 19.5 (155.9 for Tensor cores)                     |\n",
    "\n",
    "\n",
    "That's a **factor of ~31** (with tensor cores) in favor of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak memory bandwidth\n",
    "\n",
    "The peak memory bandwidth of GPUs is much higher than for CPUs (we're even considering a dual-CPU node here):\n",
    "\n",
    "|               | peak memory bandwidth [GB/s] |\n",
    "|:-------------:|:---------------:|\n",
    "| 2x AMD EPYC 7763 |  400   |\n",
    "| NVIDIA A100   | 1560 |\n",
    "\n",
    "That's a **factor of ~4** in favor of the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What matters more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio of the peak values:\n",
    "\n",
    "$$\n",
    "\\dfrac{19.5 \\ [\\textrm{TFLOPS}]}{1.56 \\ [\\textrm{TB/s}]} \\cdot 4 \\ \\textrm{B} \\approx 50\n",
    "$$\n",
    "\n",
    "That's 50 floating point operations that can be done per each FP32 number read from memory. For tensor cores, it's even 400.\n",
    "\n",
    "For most scientific codes this means that they are **memory-bound** (bound by how fast numbers can be gathered) rather than compute-bound (how fast arithmetics can be performed).\n",
    "\n",
    "‚Üí **Floating point arithmetics is essentially free!**\n",
    "\n",
    "(‚Üí exercise **saxpy_gpu** and **daxpy_cpu** etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA.jl\n",
    "\n",
    "(We'll focus on NVIDIA GPUs but there is [support](https://juliagpu.org/) for GPUs by other vendors (AMD, Intel, etc.) as well.)\n",
    "\n",
    "Relevant links:\n",
    "* [CUDA language extension](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "* [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl)\n",
    "\n",
    "**CUDA.jl provides:**\n",
    "\n",
    "* High-level abstraction `CuArray`\n",
    "* Tools for writing CUDA kernels\n",
    "* Wrappers to proprietary NVIDIA libraries (e.g. cuBLAS, cuFFT, cuSOLVER, cuSPARSE)\n",
    "\n",
    "CUDA.jl leverages LLVM to compile **native GPU code** (compare to `nvcc`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA toolkit (binary dependency)\n",
    "\n",
    "By default, the CUDA toolkit is installed automatically when using CUDA.jl for the first time.\n",
    "\n",
    "**Note:** You can readily add and precompile CUDA.jl on a machine without GPUs, say, a login node.\n",
    "\n",
    "#### Using a system-wide CUDA installation\n",
    "\n",
    "```\n",
    "CUDA.set_runtime_version!(v\"12.2\"; local_toolkit=true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.functional() # if this works, you're good to go üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device() # the currently selected GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CuArray`: High-level array programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use a GPU is via **vectorized array operations** (e.g. broadcasting). Each of these operations will be backed by one or more GPU kernels, either natively written in Julia or from some application library.\n",
    "\n",
    "You use the `CuArray` type, which serves a dual purpose:\n",
    "\n",
    "* a managed container that represents GPU memory\n",
    "* a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.rand(4) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"./imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_cpu = [1,2,3,4]\n",
    "x_gpu = CuArray(x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` or `copy!` to move it into already allocated memory)\n",
    "\n",
    "For better performance the data movement between CPU and GPU should be minimized as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array computations on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 2048\n",
    "A_gpu = CUDA.rand(N,N);\n",
    "B_gpu = CUDA.rand(N,N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu * B_gpu # we need CUDA.@sync because GPU operations are typically asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime CUDA.@sync(A_cpu * B_cpu) setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime CUDA.@sync(A_gpu * B_gpu) setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More examples: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu .+ B_gpu; # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync sqrt.(A_gpu.^2 + B_gpu.^2); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync mapreduce(sin, +, A_gpu); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power of simple GPU array programming can not be underestimated!**\n",
    "\n",
    "Entire codes (like machine learning frameworks etc.) can be ported to GPU without ever writing a single CUDA kernel manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar access contradicts the inherent parallelism of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@allowscalar A_gpu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must express arithmetic operations in terms of arrays and treat the `CuArray` array as a whole entity, e.g.\n",
    "\n",
    "```julia\n",
    "CUDA.@sync C .= A .* B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. However, the GC is CPU-focused and isn't good at sensing GPU memory pressure.\n",
    "\n",
    "By default CUDA.jl uses a **memory pool** to speed up future allocations. So it might sometimes appear as if the objects have not been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sizeof(x_gpu) |> Base.format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.pool_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` to more agressively release the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is a reason the function is named \"unsafe\": One still has the handle `x_gpu` that now points to free'd memory.\n",
    "\n",
    "Resonable application:\n",
    "\n",
    "```julia\n",
    "function myfunction(x::CuArray)\n",
    "    tmp_memory = similar(x)\n",
    "    expensive_operation!(x, tmp_memory)\n",
    "    CUDA.unsafe_free!(tmp_memory)\n",
    "    return x\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing # to be safe :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High-level array programming doesn't cover all kinds of computations and doesn't always give the (absolute) best performance. In these cases, you can manually write a CUDA kernel directly in Julia.\n",
    "\n",
    "### Our first CUDA kernel\n",
    "**CUDA kernel**: a function that will be executed by all *GPU threads* in parallel.\n",
    "\n",
    "Based on the index of a thread we can make them operate on different pieces of given data.\n",
    "\n",
    "(It might be helpful to think of the CUDA kernel as being the body of a parallel loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x # the thread index (\"loop index\")\n",
    "    x[i] += 1\n",
    "    return # CUDA kernels may not return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can (asynchronously) launch the kernel on the GPU with the `@cuda` macro. (Think of it being a batch version of `@spawn` for the GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats of kernel programming\n",
    "Kernel programming has significant limitations and can become more difficult, especially if you care about performance. Some of the reasons are:\n",
    "\n",
    "* some computations require **communication between GPU threads** (e.g. reductions)\n",
    "* you need to respect **hardware limitations** of the GPU\n",
    "* kernels execute on the GPU where the **Julia runtime isn't available**\n",
    "\n",
    "In particular due to the last point, kernel code has limitations:\n",
    "  * no GC\n",
    "  * no `print` etc. (‚Üí `@cuprint`)\n",
    "  * code must be fully type inferred (no dynamic dispatch allowed)\n",
    "  * no `try ... catch ... end`\n",
    "  * ...\n",
    "\n",
    "**You can't just write arbitrary Julia code in kernels.** Fortunately though, many things just work and can get you very far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Hardware limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025) # one more element than before\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA programming model\n",
    "\n",
    "<!-- <img src=\"./imgs/CUDA_programming_model.png\" width=1024> -->\n",
    "<img src=\"imgs/cuda_prog_model.svg\" width=1024>\n",
    "\n",
    "Conceptual mapping:\n",
    "\n",
    "* **Grid** of blocks ‚Üí entire GPU\n",
    "* **Blocks** of threads ‚Üí SMs\n",
    "* **Threads** ‚Üí CUDA cores\n",
    "\n",
    "**Note**: up to three dimensions, $(x, y, z)$, can be used to organize the thread blocks and threads in each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x # global thread index\n",
    "    if i <= length(x) # make sure that we're inbounds (c.f. \"loop\" iteration range)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execution configuration** for a CUDA kernel:\n",
    "* `threads`: number of threads **in each block**\n",
    "* `blocks`: number of blocks in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### How does our CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.rand(1024*1024);\n",
    "\n",
    "function add_one_broadcasting(x)\n",
    "    CUDA.@sync x .+ 1\n",
    "end\n",
    "\n",
    "# same CUDA kernel, but with different execution configurations\n",
    "function add_one_kernel_1024_1k(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_256_4k(x)\n",
    "    CUDA.@sync @cuda threads=256 blocks=4*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_64_16k(x)\n",
    "    CUDA.@sync @cuda threads=64 blocks=16*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "@btime add_one_broadcasting(x)   setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_1024_1k(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_256_4k(x)  setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_64_16k(x)  setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a CUDA kernel is affected by the execution configuration (because of the runtime scheduling of thread blocks and threads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### How to obtain a good execution configuration for a CUDA kernel? ‚Üí [Occupancy API](https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The occupancy API is an automatic tool that can be used to obtain *reasonably good* execution configurations.\n",
    "\n",
    "**Occupancy** measures the ratio of the number of active *warps* per SM to the maximum number of possible warps per SM.\n",
    "\n",
    "*warp*: a group of 32 parallel GPU threads executing the same instructions.\n",
    "\n",
    "* Low occupancy usually implies low performance (because of underutilized hardware resources).\n",
    "* High occupancy, however, may not necessarily imply the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = @cuda launch=false cuda_kernel_blocks!(x) # don't launch the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = CUDA.launch_configuration(kernel.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number `blocks` indicates how many blocks we would need to fully occupy the GPU. For a given input `x`, we might need fewer or more blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threads = min(length(x), config.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blocks = cld(length(x), threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the kernel with the dynamic launch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel(x; threads=threads, blocks=blocks) # calling `kernel` like a regular function with keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime CUDA.@sync(kernel(x; threads=$threads, blocks=$blocks)) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `@code_*` macros for CPU there are `@device_code_*` macros for GPU. (The GPU pendant for `@code_native` is `@device_code_ptx`, though).\n",
    "\n",
    "**PTX**: a low-level **p**arallel **t**hread e**x**ecution virtual machine and instruction set architecture used in NVIDIA CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_warntype @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_llvm debuginfo=:none @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@device_code_ptx @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Note: You can't run this part because you only have a single GPU.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a GPU node of a supercomputer there might be more than one GPU:\n",
    "\n",
    "<!-- <img src=\"./imgs/Noctua2_GPU_node.png\" width=320px> -->\n",
    "<img src=\"imgs/Noctua2_GPU_node.svg\" width=320px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Multi-GPU via Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Julia task gets its own local CUDA execution environment.\n",
    "\n",
    "Hence, it is easy to use multiple GPUs in parallel by launching GPU computations on different GPUs from different Julia tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads\n",
    "\n",
    "function gpu_computation(A, B, C)\n",
    "    for i in 1:512\n",
    "        C = A * B\n",
    "        A = B * C\n",
    "        B = C * A\n",
    "    end\n",
    "    sin.(B)\n",
    "    return B\n",
    "end\n",
    "\n",
    "function multi_gpu()\n",
    "    n = 2048\n",
    "    @sync begin\n",
    "        # Julia task for the 1st GPU\n",
    "        @spawn begin\n",
    "            device!(0) # first GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 1: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 1: done\")\n",
    "        end\n",
    "        # Julia task for the 2nd GPU\n",
    "        @spawn begin\n",
    "            device!(1) # second GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 2: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 2: done\")\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call CUDA.memory_status() for all GPUs\n",
    "for dev in devices()\n",
    "    device!(dev)\n",
    "    println()\n",
    "    CUDA.pool_status()\n",
    "end\n",
    "device!(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Outlook: Multi-GPU via MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy:** One GPU per MPI rank (Julia process)\n",
    "\n",
    "More information + exercise tomorrow (‚Üí **diffusion_2d_mpi_gpu**)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and profiling GPU code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "\n",
    "@btime CUDA.@sync A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"allocations\" here means CPU allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CUDA.@time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@time A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CUDA.@profile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile A .* B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile trace=true A .* B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [**NVTX.jl**](https://github.com/JuliaGPU/NVTX.jl) to annotate (i.e. label and colorize) code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/nsight.png\" width=800px>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
