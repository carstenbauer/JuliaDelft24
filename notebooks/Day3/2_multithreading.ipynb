{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are threads?\n",
    "Threads are **execution units within a process** that can run simultaneously. While processes are separate, threads run in a **shared memory** space (heap).\n",
    "\n",
    "<!-- <img src=\"./imgs/what-are-threads.png\" width=500px> -->\n",
    "\n",
    "<br>\n",
    "<img src=\"imgs/stack_heap_threads.svg\" width=450px>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Julia with multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Julia starts with a single *user thread*. We must tell it explicitly to start multiple user threads. There are a couple of ways to do this:\n",
    "\n",
    "* Environment variable: `export JULIA_NUM_THREADS=4`\n",
    "* Command line argument: `julia --threads 4` or `julia -t 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is currently not (easily) possible to change the number of threads at runtime!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Jupyter, we create another kernel that starts Julia with multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IJulia\n",
    "installkernel(\"Julia (4 threads)\", \"--project=@.\", env=Dict(\"JULIA_NUM_THREADS\"=>\"4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we need to **refresh the page** and select the new `Julia (4 threads) 1.10` kernel in the top right corner. (Restart Jupyter if the kernel doesn't show up.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can readily check how many threads we are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Base.Threads: nthreads\n",
    "nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User threads vs default threads\n",
    "\n",
    "Technically, the Julia process is also spawning multiple threads already in \"single-threaded\" mode, like\n",
    "* a thread for unix signal listening\n",
    "* multiple OpenBLAS threads for BLAS/LAPACK operations\n",
    "* GC threads\n",
    "\n",
    "We call the threads that we can actually run computations on *user threads* or *Julia threads*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "BLAS.get_num_threads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are my threads running?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using ThreadPinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-based multithreading\n",
    "\n",
    "Julia implements **task-based** multithreading. In this paradigm, a task - e.g. a computational piece of a code - is marked for **parallel** execution on **any** of the Julia threads. Julia's **dynamic scheduler** will put the task on a thread and trigger the execution of the task.\n",
    "\n",
    "<br>\n",
    "<!-- <img src=\"imgs/task-based-parallelism.png\" width=768px> -->\n",
    "<img src=\"imgs/tasks_threads_cores.svg\" width=650px>\n",
    "</br>\n",
    "\n",
    "Task-based multithreading: **The user should think about tasks and not threads**.\n",
    "* By default, the user does not control on which thread a task will run (the task might even [migrate](https://docs.julialang.org/en/v1/manual/multi-threading/#man-task-migration) between threads!).\n",
    "\n",
    "**Advantages:**\n",
    "* high-level abstraction: one can spawn many tasks (>> number of threads)\n",
    "* nestable multithreading\n",
    "\n",
    "**Disadvantages:**\n",
    "* dynamic scheduling overhead\n",
    "* uncertainty and potentially suboptimal task â†’ thread assignment\n",
    "  * can get in the way when performance engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@spawn 3+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@spawn` creates a `Task` and schedules it for execution on an available Julia thread (we don't control which one!).\n",
    "\n",
    "Note that `Threads.@spawn` is **asynchronous** and **non-blocking**, that is, it doesn't wait for the task to actually run but immediately returns a `Task`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fetch the result of a task with `fetch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = @spawn 3+3\n",
    "fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `@spawn` returns right away, `fetch` is **blocking** as it has to wait for the task to actually finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@time t = @spawn begin\n",
    "    sleep(3)\n",
    "    return 3+3\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the macro `@sync` to synchronize all (lexically) encompassed asynchronous operations (`@spawn`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@time @sync t = @spawn begin\n",
    "    sleep(3)\n",
    "    return 3+3\n",
    "end\n",
    "@time fetch(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: multithreaded `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "`tmap`: *threaded map*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function tmap(f, collection)\n",
    "    # for each x âˆˆ collection, spawn a task to compute f(x)\n",
    "    tasks = map(collection) do x\n",
    "        @spawn f(x)\n",
    "    end\n",
    "    # fetch and return all the results\n",
    "    return fetch.(tasks)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "M = [rand(200,200) for i in 1:8];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra: svdvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmap(svdvals, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime tmap($svdvals, $M) samples=10 evals=3;\n",
    "@btime map($svdvals, $M) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use multithreading in Julia in combination with BLAS/LAPACK functions, it is important to carefully consider and configure the [interplay between Julia threads and BLAS threads](https://carstenbauer.github.io/ThreadPinning.jl/stable/explanations/blas/).\n",
    "\n",
    "Easiest way out: turn of BLAS/LAPACK multithreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra: BLAS\n",
    "BLAS.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime tmap($svdvals, $M) samples=10 evals=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: multithreading for-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads.Tools: taskid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@sync for i in 1:8\n",
    "    @spawn println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example: nestable multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Fibonacci series\n",
    "\n",
    "$$ F(n) = F(n-1) + F(n-2), \\qquad F(1) = F(2) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: Algorithmically, this is a highly inefficient implementation of the Fibonacci series!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function fib(n)\n",
    "    n < 2 && return n\n",
    "    t = @spawn fib(n-2)\n",
    "    return fib(n-1) + fetch(t)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nesting `@spawn` calls recursively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fib(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmap(fib, 1:20) # multithreaded tmap applying a multithreaded fib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balancing and chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are many tasks (e.g. many more than available threads), Julia's scheduler balances the load of these tasks among threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads: chunks, index_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(10)\n",
    "collect(chunks(data; n=3)) # chunks hold elements of x (views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect(index_chunks(x; n=3)) # chunks hold indices of elements of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is purely pedagogical\n",
    "function tmap_tracking(f, collection; tracker = [UnitRange[] for _ in 1:nthreads()], ntasks=nthreads())\n",
    "    result = zeros(Float64, length(collection))\n",
    "    @sync for chunk_indices in index_chunks(collection; n=ntasks)   # chunk up collection into ntasks-many chunks\n",
    "        @spawn begin                                                # spawn a task for each chunk\n",
    "            for i in chunk_indices                                  # for each element of a that belongs to this chunk/task\n",
    "                result[i] = f(collection[i])                        # apply f\n",
    "            end\n",
    "            push!(tracker[threadid()], chunk_indices)               # keep track of which thread ran the task\n",
    "        end\n",
    "    end\n",
    "    return result, tracker\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = 1:2^7\n",
    "f(x) = sum(abs2, rand() for _ in 1:(2^14*x)) # computational cost is increasing as a function of x (non-uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using StatsPlots\n",
    "using Base.Threads: nthreads\n",
    "\n",
    "result, tracker = tmap_tracking(f, xs; ntasks=length(xs))   # create a task for each element of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=8*nthreads()) # create 8*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=4*nthreads()) # create 4*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=2*nthreads()) # create 2*nthreads() tasks, each handling a chunk of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=1)            # create a single task, handling all of `a`\n",
    "# result, tracker = tmap_tracking(f, xs; ntasks=nthreads())   # create nthreads() tasks, each handling a chunk of `a`\n",
    "\n",
    "# plotting\n",
    "thread_workloads = zeros(Int, nthreads(), maximum(length, tracker))\n",
    "for th in eachindex(tracker)\n",
    "    for (i, ws) in enumerate(tracker[th])\n",
    "        thread_workloads[th, i] = sum(ws)\n",
    "    end\n",
    "end\n",
    "b = groupedbar(thread_workloads, xlab=\"threadid\", ylab=\"workload\", title=\"@spawn\", legend=false, bar_position=:stack)\n",
    "display(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multithreading for-loops (revisited): `OhMyThreads.@tasks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads: @tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tasks for i in 1:8\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The iteration space is divided into `nthreads()` contiguous chunks**, then creates a task for each chunks. $\\quad \\Rightarrow \\quad $ **no load balancing!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tune the number of tasks to spawn (== chunking granularity) for `@tasks` with `@set ntasks = value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads: @set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tasks for i in 1:8\n",
    "    @set ntasks = 1\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tasks for i in 1:8\n",
    "    @set ntasks = 8   # same as @sync for .... @spawn ... end\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that you can't tune the number of tasks for `Threads.@threads`! ðŸ™)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Opting out of dynamic scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "For \"traditional HPC\", where you tell each thread what to do, you might want to opt out of dynamic scheduling and task migration. \n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* guaranteed task-thread mapping (\"task pinning\")\n",
    "* lower overhead\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* often less portable code (e.g. hardcoded assumptions about the system)\n",
    "* no (or at least bad) nestability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spawning a sticky task on a specific thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using OhMyThreads: @spawnat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@spawnat 4 println(\"Task \", taskid(), \" is running on thread \", threadid(), \", and always will be ðŸ˜‰\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Static scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Statically** map tasks to threads, specifically: task 1 â†’ thread 1, task 2 â†’ thread 2, and so on.\n",
    "\n",
    "For `@tasks` there is `@set scheduler = :static`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tasks for i in 1:2*nthreads()\n",
    "    @set scheduler = :static\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid());\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `scheduler = :static`, every thread handles precisely two iterations and always the same iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tasks for i in 1:2*nthreads()\n",
    "    @set scheduler = :dynamic # :dynamic is the default\n",
    "    println(\"Task \", taskid(), \" is running iteration \", i, \" on thread \", threadid());\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beware of Multithreading: Parallel Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = rand(1_000_000 * nthreads());\n",
    "\n",
    "sum(data) # we want to parallelize this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How you should parallelize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real answer is: There is no need to roll your own parallel summation (or your own `tmap` ðŸ˜‰). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OhMyThreads: treduce\n",
    "\n",
    "treduce(+, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treduce(+, data) â‰ˆ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's assume we want to write a parallel version ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-focused parallel version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key questions for task-based parallelisation:\n",
    "* How to divide the computation into seperate **tasks**?\n",
    "    * Answer: chunk up the data and perform partial sums.\n",
    "* How many **tasks** should we create?\n",
    "    * Answer: since the workload is uniform, `nthreads()` many tasks is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_map_spawn(data; ntasks=nthreads())\n",
    "    ts = map(chunks(data, n=ntasks)) do chunk_elements\n",
    "        @spawn sum(chunk_elements)\n",
    "    end\n",
    "    return sum(fetch.(ts))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptually simple and task-focused\n",
    "  * â†’ We're **explicitly** spawning one task per chunk.\n",
    "  * â†’ No mention of threads, except in `ntasks=nthreads()`.\n",
    "* In the latter form, we don't even need a manual pre-allocation (it is hidden in the map operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_map_spawn(data) â‰ˆ sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime sum_map_spawn($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 1: Race condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_naive(data)\n",
    "    s = zero(eltype(data))\n",
    "    @tasks for i in eachindex(data)\n",
    "        s += data[i]\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@show sum(data);\n",
    "@show sum_threads_naive(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wrong** result! Even worse, it's **non-deterministic** and different every time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a [race condition](https://en.wikipedia.org/wiki/Race_condition) which typically appear when multiple tasks are modifying shared state simultaneously.\n",
    "\n",
    "â†’ If possible, **don't modify shared (i.e. non task-local) state!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Thread-focused rather than task-focused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be inclined to write something similar to the following (intentionally written in a slightly more verbose form):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_unsafe(data)\n",
    "    psums = zeros(eltype(data), nthreads())\n",
    "    @threads for i in eachindex(data)    # spawn nthreads many tasks\n",
    "        current_sum = psums[threadid()]  # read\n",
    "        new_sum = current_sum + data[i]  # \"work\"\n",
    "        psums[threadid()] = new_sum      # write\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such an approach is generally **unsafe** because Julia's scheduler may **migrate tasks between threads**!\n",
    "  * For example, a task might start on thread 1, is then paused (say, after \"work\") and migrated to thread 3, where it finishes execution.\n",
    "  * â†’ The output of `threadid()` might change within a task! To be safe, [don't use `threadid()`](https://julialang.org/blog/2023/07/PSA-dont-use-threadid/) at all!\n",
    "  \n",
    "It also goes against the idea of task-based multithreading, as we're **thinking about threads rather than tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that, in spite of the comments above, the `threadid()` pattern will often still work correctly. This is because as of Julia 1.10 task migrations are very rare. **You can't rely on it though!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_threads_unsafe(data) â‰ˆ sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Performance) Mistake 3: False sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_chunks(data; nchunks=nthreads())\n",
    "    psums = zeros(eltype(data), nchunks)\n",
    "    @tasks for (c, chunk_elements) in enumerate(chunks(data; n=nchunks)) # spawn nchunks many tasks\n",
    "        @simd for x in chunk_elements\n",
    "            psums[c] += x\n",
    "        end\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum_threads_chunks(data) â‰ˆ sum(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum($data);\n",
    "@btime sum_threads_chunks($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe, but slow?! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performance issue: [False sharing](https://en.wikipedia.org/wiki/False_sharing)\n",
    "\n",
    "Why does `sum_threads_chunks` above have bad performance? Although argubaly subtle, this is because different tasks mutate shared data (`psums`) in parallel. There is no *logical* sharing: Tasks access different slots of `psums` and there is no data race. However, CPU cores work on the basis of **cache lines** instead of single elements leading to *implicit* sharing of cache lines.\n",
    "\n",
    "**Despite its subtlety, false sharing can lead to dramatic slowdown!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CpuId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cachelinesize() Ã· sizeof(Float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/false_sharing.svg\" width=850px>\n",
    "\n",
    "Different tasks modify the same cache line\n",
    "* need for synchronization to ensure cache coherency\n",
    "* performance decreases (dramatically).\n",
    "\n",
    "Once agin: **The less you modify shared (i.e. non task-local) state, the better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \"Fixed\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sum_threads_chunks_local(data; nchunks=nthreads())\n",
    "    psums = zeros(eltype(data), nchunks)\n",
    "    @tasks for (c, chunk_elements) in enumerate(chunks(data; n=nchunks))  # spawn nchunks many tasks\n",
    "        psums[c] = sum(chunk_elements)\n",
    "    end\n",
    "    return sum(psums)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* each task/iteration computes a local sum independently\n",
    "* no *frequent* non-local mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sum(data) â‰ˆ sum_threads_chunks_local(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@btime sum($data);\n",
    "@btime sum_threads_chunks_local($data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronization/communication of tasks\n",
    "\n",
    "Low-level to high-level (roughly):\n",
    "\n",
    "* [atomic operations](https://docs.julialang.org/en/v1/base/multi-threading/#Atomic-operations)\n",
    "* [locks](https://docs.julialang.org/en/v1/base/parallel/#Base.ReentrantLock)\n",
    "* [channels](https://docs.julialang.org/en/v1/base/parallel/#Channels)\n",
    "\n",
    "Generally, one should try to minimize synchronization/communication as much as possible as it can lead to serialization (or worse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dict()\n",
    "lck = ReentrantLock()\n",
    "@tasks for i in 1:1000\n",
    "    @lock lck d[i] = i\n",
    "end\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage collection\n",
    "\n",
    "If it gets triggered, it stops the world (all threads) for clearing up memory.\n",
    "\n",
    "Hence, when using multithreading, it is even more important to **avoid heap allocations!**\n",
    "\n",
    "(If you can't avoid allocations, consider using multiprocessing instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinning Julia threads to CPU threads/cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compute node has a complex topology (two sockets, multiple memory channels/domains). Placing the Julia threads systematically on CPU-threads matters for\n",
    "\n",
    "* the computation performance of your Julia codes\n",
    "* fluctuations/noises in benchmarks\n",
    "* hardware-level performance monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ThreadPinning.jl\n",
    "\n",
    "`pinthreads(strategy)`\n",
    "* `:cputhreads` pin to CPU threads (incl. \"hypterthreads\") one after another\n",
    "* `:cores:` pin to CPU cores one after another\n",
    "* `:numa:` round-robin between NUMA domains\n",
    "* `:sockets:` round-robin between sockets\n",
    "* `:affinitymask`: according to an external affinity mask (e.g. set by SLURM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threadinfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinthreads(:cores)\n",
    "threadinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll explore the effect of thread pinning on performance in more detail later â†’ **daxpy_cpu exercise**"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia (4 threads) 1.10.5",
   "language": "julia",
   "name": "julia-_4-threads_-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
